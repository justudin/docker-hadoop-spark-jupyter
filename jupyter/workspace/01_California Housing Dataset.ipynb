{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical implementation\n",
    "Now, let’s delve into coding to understand the practical\n",
    "implementation of regression system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "The California Housing dataset is a popular dataset that’s used in\n",
    "data science and machine learning to demonstrate algorithms and\n",
    "techniques for regression analysis. This dataset typically serves as a\n",
    "benchmark for predictive modeling tasks, where the goal is to\n",
    "forecast median house values in Californian districts based on\n",
    "various features. Originating from the 1990s, it was compiled for a\n",
    "study related to housing needs in California and has since become a\n",
    "staple example in machine learning communities, particularly for\n",
    "those starting with data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The California Housing dataset is widely used for teaching and\n",
    "testing regression models and is a type of predictive modeling\n",
    "technique that’s used to understand relationships between\n",
    "independent variables (features) and a continuous dependent\n",
    "variable (target). In the context of this dataset, regression models\n",
    "are built to predict the median house value based on district-level\n",
    "characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the California Housing data\n",
    "The following lines of code mainly focus on loading the dataset,\n",
    "converting it into a pandas DataFrame, and then creating a Spark\n",
    "DataFrame from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b773ab5-e13c-469c-a626-3e510959c7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 18:14:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/13 18:14:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RegressionPipelineExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code snippet will output the content in DataFrame\n",
    "format, as shown here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+\n",
      "|MedInc|HouseAge|          AveRooms|         AveBedrms|Population|          AveOccup|Latitude|Longitude|label|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+\n",
      "|8.3252|    41.0| 6.984126984126984|1.0238095238095237|     322.0|2.5555555555555554|   37.88|  -122.23|4.526|\n",
      "|8.3014|    21.0| 6.238137082601054|0.9718804920913884|    2401.0| 2.109841827768014|   37.86|  -122.22|3.585|\n",
      "|7.2574|    52.0| 8.288135593220339| 1.073446327683616|     496.0|2.8022598870056497|   37.85|  -122.24|3.521|\n",
      "|5.6431|    52.0|5.8173515981735155|1.0730593607305936|     558.0| 2.547945205479452|   37.85|  -122.25|3.413|\n",
      "|3.8462|    52.0| 6.281853281853282|1.0810810810810811|     565.0|2.1814671814671813|   37.85|  -122.25|3.422|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data=housing.data,columns=housing.feature_names)\n",
    "df['label'] = housing.target\n",
    "df_data = spark.createDataFrame(df)\n",
    "df_data.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet prepares the dataset for machine\n",
    "learning by transforming the feature columns into a format\n",
    "compatible with Spark MLlib’s algorithms, allowing the models to\n",
    "be trained on large-scale data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed223fb-e71e-4e64-991f-5172f28fcf46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+-----------------------------------------------------------------------------------------+\n",
      "|MedInc|HouseAge|          AveRooms|         AveBedrms|Population|          AveOccup|Latitude|Longitude|label|                                                                                 features|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+-----------------------------------------------------------------------------------------+\n",
      "|8.3252|    41.0| 6.984126984126984|1.0238095238095237|     322.0|2.5555555555555554|   37.88|  -122.23|4.526|[8.3252,41.0,6.984126984126984,1.0238095238095237,322.0,2.5555555555555554,37.88,-122.23]|\n",
      "|8.3014|    21.0| 6.238137082601054|0.9718804920913884|    2401.0| 2.109841827768014|   37.86|  -122.22|3.585|[8.3014,21.0,6.238137082601054,0.9718804920913884,2401.0,2.109841827768014,37.86,-122.22]|\n",
      "|7.2574|    52.0| 8.288135593220339| 1.073446327683616|     496.0|2.8022598870056497|   37.85|  -122.24|3.521| [7.2574,52.0,8.288135593220339,1.073446327683616,496.0,2.8022598870056497,37.85,-122.24]|\n",
      "|5.6431|    52.0|5.8173515981735155|1.0730593607305936|     558.0| 2.547945205479452|   37.85|  -122.25|3.413|[5.6431,52.0,5.8173515981735155,1.0730593607305936,558.0,2.547945205479452,37.85,-122.25]|\n",
      "|3.8462|    52.0| 6.281853281853282|1.0810810810810811|     565.0|2.1814671814671813|   37.85|  -122.25|3.422|[3.8462,52.0,6.281853281853282,1.0810810810810811,565.0,2.1814671814671813,37.85,-122.25]|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+-----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the features column\n",
    "feature_cols = df_data.columns[:-1]  # Assuming the last column is the label\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_data = assembler.transform(df_data)\n",
    "df_data.show(5,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling and normalization are important preprocessing\n",
    "steps in the data preparation phase of machine learning and data\n",
    "modeling. These techniques adjust the scale or distribution of\n",
    "features (variables) in your data, improving model performance,\n",
    "ensuring faster convergence, and providing balanced regularization.\n",
    "The following code snippet uses StandardScaler, a feature\n",
    "transformation tool used for scaling and normalizing features in\n",
    "data preprocessing for machine learning models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "685bb471-2391-49c0-871d-48e426a52511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scale and normalize features\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data is a fundamental practice in machine learning for\n",
    "validating models. It helps prevent overfitting, where a model\n",
    "might perform well on the training data but poorly on new, unseen\n",
    "data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5106b59a-b9ab-40d7-abd1-91ca14fb7837",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 18:14:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(scavenge), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/11/13 18:14:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(global, scavenge), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "(trainingData, testData) = df_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing machine learning models, such as the ones in our code\n",
    "snippet, is a crucial step in the machine learning workflow.\n",
    "The following code snippet initializes five different regression\n",
    "models from Apache Spark’s MLlib, each designed for different\n",
    "types of regression tasks. All these models are configured with a\n",
    "specified features column named features and a label column\n",
    "named label. In Spark MLlib, the featuresCol parameter is\n",
    "used to specify the input column that contains feature vectors, and\n",
    "the labelCol parameter specifies the column containing the target\n",
    "variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize regression models\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "glr = GeneralizedLinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines in Spark MLlib are powerful tools for building machine\n",
    "learning workflows. They allow you to chain multiple\n",
    "transformation and model training stages, ensuring that all the steps\n",
    "are executed in the correct order. This is particularly useful for data\n",
    "preprocessing, feature transformation, and model training and\n",
    "evaluation to be bundled together into a single, reusable workflow.\n",
    "Pipelines also help in maintaining clean code and facilitate the\n",
    "deployment and reuse of machine learning models.\n",
    "The following code snippet creates five separate pipelines using\n",
    "Apache Spark’s Pipeline API, with each pipeline containing one of\n",
    "the machine learning regression models shown previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline for each regression algorithm\n",
    "pipeline_lr = Pipeline(stages=[lr])\n",
    "pipeline_glr = Pipeline(stages=[glr])\n",
    "pipeline_dt = Pipeline(stages=[dt])\n",
    "pipeline_rf = Pipeline(stages=[rf])\n",
    "pipeline_gbt = Pipeline(stages=[gbt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet trains the machine learning models on\n",
    "a dataset. Each line of code fits a different regression model to the\n",
    "training data, creating trained models that can be used for\n",
    "predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 18:15:03 WARN Instrumentation: [47478497] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/13 18:15:03 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "25/11/13 18:15:03 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "25/11/13 18:15:04 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "25/11/13 18:15:04 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "25/11/13 18:15:05 WARN Instrumentation: [0532d87c] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipelines\n",
    "model_lr = pipeline_lr.fit(trainingData)\n",
    "model_glr = pipeline_glr.fit(trainingData)\n",
    "model_dt = pipeline_dt.fit(trainingData)\n",
    "model_rf = pipeline_rf.fit(trainingData)\n",
    "model_gbt = pipeline_gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet takes the model that’s been trained on\n",
    "the training data and applies it to unseen test data to evaluate the\n",
    "model’s performance. The transform method in Spark MLlib is\n",
    "used for this purpose, and it adds a column (typically named\n",
    "prediction) to the input DataFrame that contains the predicted\n",
    "values for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_lr = model_lr.transform(testData)\n",
    "predictions_glr = model_glr.transform(testData)\n",
    "predictions_dt = model_dt.transform(testData)\n",
    "predictions_rf = model_rf.transform(testData)\n",
    "predictions_gbt = model_gbt.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet evaluates the performance of different\n",
    "regression models on the test dataset using the RMSE metric:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48d1307-c849-4ad5-9bf2-a19cc33314ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 1.1394418865304807\n",
      "General Linear Regression RMSE: 1.1394418865298508\n",
      "Decision Tree Regression RMSE: 0.7237963286049708\n",
      "Random Forest Regression RMSE: 0.6875829969719259\n",
      "Gradient Boosted Tree Regression RMSE: 0.5676564538704333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse_lr = evaluator.evaluate(predictions_lr)\n",
    "rmse_glr = evaluator.evaluate(predictions_glr)\n",
    "rmse_dt = evaluator.evaluate(predictions_dt)\n",
    "rmse_rf = evaluator.evaluate(predictions_rf)\n",
    "rmse_gbt = evaluator.evaluate(predictions_gbt)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", rmse_lr)\n",
    "print(\"General Linear Regression RMSE:\", rmse_glr)\n",
    "print(\"Decision Tree Regression RMSE:\", rmse_dt)\n",
    "print(\"Random Forest Regression RMSE:\", rmse_rf)\n",
    "print(\"Gradient Boosted Tree Regression RMSE:\", rmse_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bbd44f-16fc-4f43-a333-644da044981d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+------------------------------------------------------------+------------------+\n",
      "|MedInc|HouseAge|          AveRooms|         AveBedrms|Population|          AveOccup|Latitude|Longitude|label|                                                    features|        prediction|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+------------------------------------------------------------+------------------+\n",
      "|0.4999|    46.0|1.7142857142857142|0.5714285714285714|      18.0|2.5714285714285716|   37.81|  -122.29|0.675|[0.4999,46.0,1.7142857142857142,0.5714285714285714,18.0,2...| 1.062330373214671|\n",
      "|0.8172|    52.0| 6.102459016393443|1.3729508196721312|     728.0|2.9836065573770494|   37.82|  -122.28|0.853|[0.8172,52.0,6.102459016393443,1.3729508196721312,728.0,2...|1.3010006477491558|\n",
      "|0.8668|    52.0|2.4431818181818183|0.9886363636363636|     904.0|10.272727272727273|    37.8|  -122.28|1.375|[0.8668,52.0,2.4431818181818183,0.9886363636363636,904.0,...|1.2053416794938414|\n",
      "|0.9011|    50.0| 6.229508196721311|1.5573770491803278|     377.0|3.0901639344262297|   37.81|  -122.29|0.861|[0.9011,50.0,6.229508196721311,1.5573770491803278,377.0,3...|1.4448791232486684|\n",
      "|0.9218|    21.0| 2.045662100456621|1.0342465753424657|     735.0| 1.678082191780822|   37.82|  -122.27|1.719|[0.9218,21.0,2.045662100456621,1.0342465753424657,735.0,1...|1.3437038459737067|\n",
      "+------+--------+------------------+------------------+----------+------------------+--------+---------+-----+------------------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_lr.show(5,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Regression analysis plays a crucial role in supervised learning,\n",
    "enabling us to understand relationships between variables and\n",
    "make informed predictions."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "California Housing Dataset",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9526f826",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b07fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ceff9",
   "metadata": {},
   "source": [
    "### import the required libraries and create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b91a71b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the library and create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ClassificationExample\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e8654",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data and create dataframe\n",
    "### Display the first 3 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9e9ce38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----------------+---------------------+--------------------+---------+-------------+---------------+-----------------+------------------------+-------------+------------------+---------------+\n",
      "|UDI|Product ID|type|air_temperature_k|process_temperature_k|rotational_speed_rpm|torque_nm|tool_wear_min|machine_failure|tool_wear_failure|heat_dissipation_failure|power_failure|overstrain_failure|random_failures|\n",
      "+---+----------+----+-----------------+---------------------+--------------------+---------+-------------+---------------+-----------------+------------------------+-------------+------------------+---------------+\n",
      "|1  |M14860    |M   |298.1            |308.6                |1551                |42.8     |0            |0              |0                |0                       |0            |0                 |0              |\n",
      "|2  |L47181    |L   |298.2            |308.7                |1408                |46.3     |3            |0              |0                |0                       |0            |0                 |0              |\n",
      "|3  |L47182    |L   |298.1            |308.5                |1498                |49.4     |5            |0              |0                |0                       |0            |0                 |0              |\n",
      "+---+----------+----+-----------------+---------------------+--------------------+---------+-------------+---------------+-----------------+------------------------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.csv(\"machine_failure_data.csv\", header=True, inferSchema=True)\n",
    "train_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef74723",
   "metadata": {},
   "source": [
    "### Preprocessing the data: Convert all integer columns to float, remove null values and print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d2f3327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UDI: float (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- air_temperature_k: double (nullable = true)\n",
      " |-- process_temperature_k: double (nullable = true)\n",
      " |-- rotational_speed_rpm: float (nullable = true)\n",
      " |-- torque_nm: double (nullable = true)\n",
      " |-- tool_wear_min: float (nullable = true)\n",
      " |-- machine_failure: float (nullable = true)\n",
      " |-- tool_wear_failure: float (nullable = true)\n",
      " |-- heat_dissipation_failure: float (nullable = true)\n",
      " |-- power_failure: float (nullable = true)\n",
      " |-- overstrain_failure: float (nullable = true)\n",
      " |-- random_failures: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all integer columns in train_df to float\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "for col in train_df.columns:\n",
    "    if train_df.schema[col].dataType == IntegerType():\n",
    "        train_df = train_df.withColumn(col, train_df[col].cast(FloatType()))\n",
    "# remove null values\n",
    "train_df = train_df.dropna()\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b4f24",
   "metadata": {},
   "source": [
    "# Prepare the data by removing the id columns and failure columns retaining only the machine failure column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25d74bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+\n",
      "|type|air_temperature_k|process_temperature_k|rotational_speed_rpm|torque_nm|tool_wear_min|machine_failure|\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+\n",
      "|M   |298.1            |308.6                |1551.0              |42.8     |0.0          |0.0            |\n",
      "|L   |298.2            |308.7                |1408.0              |46.3     |3.0          |0.0            |\n",
      "|L   |298.1            |308.5                |1498.0              |49.4     |5.0          |0.0            |\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drope the id columns such as UDI, Product ID,heat_dissipation_failure,power_failure,overstrain_failure,random_failures,tool_wear_failure\n",
    "train_df = train_df.drop(\"UDI\",\"Product ID\",\"heat_dissipation_failure\",\"power_failure\",\"overstrain_failure\",\"random_failures\",\"tool_wear_failure\")\n",
    "train_df.show(3,truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639afa41",
   "metadata": {},
   "source": [
    "# Feature Engineering: Fill missing values, convert the type column to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "879322b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+----------+\n",
      "|type|air_temperature_k|process_temperature_k|rotational_speed_rpm|torque_nm|tool_wear_min|machine_failure|type_index|\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+----------+\n",
      "|   M|            298.1|                308.6|              1551.0|     42.8|          0.0|            0.0|       1.0|\n",
      "|   L|            298.2|                308.7|              1408.0|     46.3|          3.0|            0.0|       0.0|\n",
      "|   L|            298.1|                308.5|              1498.0|     49.4|          5.0|            0.0|       0.0|\n",
      "+----+-----------------+---------------------+--------------------+---------+-------------+---------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace  the missing values with 0\n",
    "train_df = train_df.fillna(0)\n",
    "# convert the type column to index\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "train_df = indexer.fit(train_df).transform(train_df)\n",
    "train_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7394e6",
   "metadata": {},
   "source": [
    "### Feature Engineering using vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc177db1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            features|machine_failure|\n",
      "+--------------------+---------------+\n",
      "|[298.1,308.6,1551...|            0.0|\n",
      "|[298.2,308.7,1408...|            0.0|\n",
      "|[298.1,308.5,1498...|            0.0|\n",
      "+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"air_temperature_k\", \"process_temperature_k\", \"rotational_speed_rpm\", \"torque_nm\", \"tool_wear_min\",\"type_index\"], outputCol=\"features\")\n",
    "train_df = assembler.transform(train_df)\n",
    "train_df.select(\"features\", \"machine_failure\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f3e21",
   "metadata": {},
   "source": [
    "### Applying standard scaler and show the scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed48337c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|      scaledFeatures|machine_failure|\n",
      "+--------------------+---------------+\n",
      "|[149.030724148837...|            0.0|\n",
      "|[149.080717682600...|            0.0|\n",
      "|[149.030724148837...|            0.0|\n",
      "+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply standard scaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(train_df)\n",
    "train_df = scalerModel.transform(train_df)\n",
    "# show the scaled features\n",
    "train_df.select(\"scaledFeatures\", \"machine_failure\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c91f8",
   "metadata": {},
   "source": [
    "### Split the data into train and test and select the features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ba42b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "train_df, test_df = train_df.randomSplit([0.7, 0.3])\n",
    "# select the features and label\n",
    "train_df = train_df.select(\"scaledFeatures\", \"machine_failure\")\n",
    "test_df = test_df.select(\"scaledFeatures\", \"machine_failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c67a53e",
   "metadata": {},
   "source": [
    "### Train the model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae240617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 14:26:07 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"machine_failure\")\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6d1cb",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf9c581",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8912492539369147"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"machine_failure\")\n",
    "evaluator.evaluate(lrModel.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b2874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
